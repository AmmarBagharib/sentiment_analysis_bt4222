{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adarsh\\AppData\\Local\\Temp\\ipykernel_38600\\1168416562.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df['sentiment'] = valid_df['label'].apply(assign_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss: 0.6145607233047485 val_loss: 0.7047645449638367\n",
      "train_accuracy: 82.01058201058201 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss: 0.5367457158863544 val_loss: 0.557709738612175\n",
      "train_accuracy: 82.27513227513228 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss: 0.4736872501671314 val_loss: 0.8566180169582367\n",
      "train_accuracy: 82.27513227513228 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss: 0.46337663009762764 val_loss: 0.5234310477972031\n",
      "train_accuracy: 81.74603174603175 val_accuracy: 82.10526315789474\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss: 0.34272936172783375 val_loss: 0.5522213876247406\n",
      "train_accuracy: 79.36507936507937 val_accuracy: 81.05263157894737\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss: 0.28280456084758043 val_loss: 0.6185223460197449\n",
      "train_accuracy: 75.92592592592592 val_accuracy: 82.10526315789474\n",
      "==================================================\n",
      "Epoch 7\n",
      "train_loss: 0.20807126304134727 val_loss: 0.5651834607124329\n",
      "train_accuracy: 82.8042328042328 val_accuracy: 74.73684210526315\n",
      "==================================================\n",
      "Epoch 8\n",
      "train_loss: 0.03989373636431992 val_loss: 0.7888500988483429\n",
      "train_accuracy: 83.5978835978836 val_accuracy: 73.68421052631578\n",
      "==================================================\n",
      "Epoch 9\n",
      "train_loss: -0.09387407288886607 val_loss: 0.7505036890506744\n",
      "train_accuracy: 83.06878306878306 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 10\n",
      "train_loss: -0.3917563669383526 val_loss: 0.8898918926715851\n",
      "train_accuracy: 83.33333333333334 val_accuracy: 72.63157894736842\n",
      "==================================================\n",
      "Epoch 11\n",
      "train_loss: -0.8391492366790771 val_loss: 1.3329633176326752\n",
      "train_accuracy: 87.03703703703704 val_accuracy: 77.89473684210526\n",
      "==================================================\n",
      "Epoch 12\n",
      "train_loss: -1.0294912680983543 val_loss: 1.6186142563819885\n",
      "train_accuracy: 86.77248677248677 val_accuracy: 72.63157894736842\n",
      "==================================================\n",
      "Epoch 13\n",
      "train_loss: -1.3486360032111406 val_loss: 1.5995643138885498\n",
      "train_accuracy: 87.3015873015873 val_accuracy: 78.94736842105263\n",
      "==================================================\n",
      "Epoch 14\n",
      "train_loss: -1.2686263690702617 val_loss: 1.2309671640396118\n",
      "train_accuracy: 86.24338624338624 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 15\n",
      "train_loss: -1.4896467328071594 val_loss: 0.9620644748210907\n",
      "train_accuracy: 87.3015873015873 val_accuracy: 75.78947368421053\n",
      "==================================================\n",
      "Epoch 16\n",
      "train_loss: -1.9323500394821167 val_loss: 4.524820804595947\n",
      "train_accuracy: 90.21164021164022 val_accuracy: 70.52631578947368\n",
      "==================================================\n",
      "Epoch 17\n",
      "train_loss: 1.8124404549598694 val_loss: 7.386038541793823\n",
      "train_accuracy: 77.24867724867724 val_accuracy: 62.10526315789474\n",
      "==================================================\n",
      "Epoch 18\n",
      "train_loss: 2.9014505594968796 val_loss: 5.301323294639587\n",
      "train_accuracy: 74.07407407407408 val_accuracy: 68.42105263157895\n",
      "==================================================\n",
      "Epoch 19\n",
      "train_loss: 0.7434840090572834 val_loss: 0.9719489812850952\n",
      "train_accuracy: 80.15873015873017 val_accuracy: 76.84210526315789\n",
      "==================================================\n",
      "Epoch 20\n",
      "train_loss: -1.4640976153314114 val_loss: 3.148545742034912\n",
      "train_accuracy: 87.3015873015873 val_accuracy: 69.47368421052632\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if CUDA is available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If a GPU is available, set the device to GPU, otherwise, use CPU\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"cleaned_village-hotel-changi-by-far-east-hospitality.csv\")\n",
    "\n",
    "# Filter out rows with NaN values in the rating column\n",
    "valid_df = df.dropna(subset=['label'])\n",
    "\n",
    "# Define a function to assign labels based on rating values\n",
    "def assign_labels(rating):\n",
    "    if rating == \"Positive\":\n",
    "        return 1  # Positive\n",
    "    elif rating == \"Negative\":\n",
    "        return -1  # Negative\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "\n",
    "# Apply the function to create a new 'sentiment' column\n",
    "valid_df['sentiment'] = valid_df['label'].apply(assign_labels)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = valid_df['combined_review'].values\n",
    "y = valid_df['sentiment'].values\n",
    "\n",
    "# Tokenization and Preprocessing (modify as needed)\n",
    "def preprocess_string(s):\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    s = re.sub(r\"\\s+\", ' ', s)\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "    return s\n",
    "\n",
    "def tokenize(x_data):\n",
    "    word_list = []\n",
    "    for sent in x_data:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word != '':\n",
    "                word_list.append(word)\n",
    "\n",
    "    corpus = Counter(word_list)\n",
    "    corpus_ = sorted(corpus, key=corpus.get, reverse=True)[:1000]\n",
    "    onehot_dict = {w: i + 1 for i, w in enumerate(corpus_)}\n",
    "\n",
    "    final_list = []\n",
    "    for sent in x_data:\n",
    "        final_list.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
    "                           if preprocess_string(word) in onehot_dict.keys()])\n",
    "\n",
    "    return np.array(final_list, dtype=object), onehot_dict\n",
    "\n",
    "x_data, vocab = tokenize(X)\n",
    "\n",
    "# Hyperparameters (modify as needed)\n",
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1  # Extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "lr = 0.001\n",
    "batch_size = 50\n",
    "\n",
    "# Padding\n",
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "x_data_pad = padding_(x_data, 500)\n",
    "y_data = y\n",
    "\n",
    "# Create Tensor datasets\n",
    "data = TensorDataset(torch.from_numpy(x_data_pad), torch.from_numpy(y_data))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Model structure\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim, num_layers=no_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)  # Calculate the actual batch size\n",
    "        # Initialize the hidden state based on the current batch size\n",
    "        h0 = torch.zeros(self.no_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.no_layers, batch_size, self.hidden_dim).to(device)\n",
    "        hidden = (h0, c0)\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden\n",
    "\n",
    "# Model and optimizer\n",
    "model = SentimentRNN(no_layers, vocab_size, hidden_dim, embedding_dim)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Function to predict accuracy\n",
    "def acc(pred, label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "# Training and Evaluation\n",
    "clip = 5\n",
    "epochs = 20\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc, epoch_vl_acc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        accuracy = acc(output, labels)\n",
    "        train_acc += accuracy\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    for inputs, labels in test_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, val_h = model(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "        val_losses.append(val_loss.item())\n",
    "        accuracy = acc(output, labels)\n",
    "        val_acc += accuracy\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(test_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    print(f'train_loss: {epoch_train_loss} val_loss: {epoch_val_loss}')\n",
    "    print(f'train_accuracy: {epoch_train_acc * 100} val_accuracy: {epoch_val_acc * 100}')\n",
    "    print(25 * '==')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adarsh\\AppData\\Local\\Temp\\ipykernel_14352\\2533021220.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df['sentiment'] = valid_df['label'].apply(assign_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss: 0.6493656486272812 val_loss: 0.4715089201927185\n",
      "train_accuracy: 73.28042328042328 val_accuracy: 85.26315789473684\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss: 0.5472150258719921 val_loss: 0.5086666643619537\n",
      "train_accuracy: 80.95238095238095 val_accuracy: 85.26315789473684\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss: 0.47524965554475784 val_loss: 0.4235513359308243\n",
      "train_accuracy: 81.48148148148148 val_accuracy: 84.21052631578947\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss: 0.5058940704911947 val_loss: 0.49164576828479767\n",
      "train_accuracy: 80.15873015873017 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss: 0.3480592295527458 val_loss: 0.4467100650072098\n",
      "train_accuracy: 78.83597883597884 val_accuracy: 80.0\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss: 0.2473376113921404 val_loss: 0.5056129395961761\n",
      "train_accuracy: 78.83597883597884 val_accuracy: 75.78947368421053\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Check if CUDA is available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If a GPU is available, set the device to GPU, otherwise, use CPU\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"cleaned_village-hotel-changi-by-far-east-hospitality.csv\")\n",
    "\n",
    "# Filter out rows with NaN values in the rating column\n",
    "valid_df = df.dropna(subset=['label'])\n",
    "\n",
    "# Define a function to assign labels based on rating values\n",
    "def assign_labels(rating):\n",
    "    if rating == \"Positive\":\n",
    "        return 1  # Positive\n",
    "    elif rating == \"Negative\":\n",
    "        return -1  # Negative\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "\n",
    "# Apply the function to create a new 'sentiment' column\n",
    "valid_df['sentiment'] = valid_df['label'].apply(assign_labels)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = valid_df['cleaned_review'].values\n",
    "y = valid_df['sentiment'].values\n",
    "\n",
    "# Tokenization and Preprocessing (modify as needed)\n",
    "def preprocess_string(s):\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    s = re.sub(r\"\\s+\", ' ', s)\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "    return s\n",
    "\n",
    "def tokenize(x_data):\n",
    "    word_list = []\n",
    "    for sent in x_data:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word != '':\n",
    "                word_list.append(word)\n",
    "\n",
    "    corpus = Counter(word_list)\n",
    "    corpus_ = sorted(corpus, key=corpus.get, reverse=True)[:1000]\n",
    "    onehot_dict = {w: i + 1 for i, w in enumerate(corpus_)}\n",
    "\n",
    "    final_list = []\n",
    "    for sent in x_data:\n",
    "        final_list.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split()\n",
    "                           if preprocess_string(word) in onehot_dict.keys()])\n",
    "\n",
    "    return np.array(final_list, dtype=object), onehot_dict\n",
    "\n",
    "x_data, vocab = tokenize(X)\n",
    "\n",
    "# Function to train the LDA model\n",
    "def train_lda(x_data, num_topics=5):\n",
    "    processed_data = [' '.join(map(str, doc)) for doc in x_data]  # Convert tokenized data to strings\n",
    "    texts = [doc.split() for doc in processed_data]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "    return lda_model\n",
    "\n",
    "lda_model = train_lda(x_data, num_topics=5)\n",
    "\n",
    "# Function to extract LDA features\n",
    "def extract_lda_features(lda_model, x_data, num_topics):\n",
    "    lda_features = []\n",
    "    for doc in x_data:\n",
    "        doc_str = ' '.join(map(str, doc))\n",
    "        bow = lda_model.id2word.doc2bow(doc_str.split())\n",
    "        topic_distribution = lda_model.get_document_topics(bow)\n",
    "        topic_distribution = [score for _, score in topic_distribution]\n",
    "\n",
    "        # Pad the topic distribution to have a fixed length (num_topics)\n",
    "        topic_distribution += [0.0] * (num_topics - len(topic_distribution))\n",
    "\n",
    "        lda_features.append(topic_distribution)\n",
    "\n",
    "    return np.array(lda_features)\n",
    "\n",
    "# Usage: Pass the 'num_topics' as an argument\n",
    "lda_features = extract_lda_features(lda_model, x_data, num_topics=5)\n",
    "\n",
    "# Hyperparameters (modify as needed)\n",
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1  # Extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "lr = 0.001\n",
    "batch_size = 50\n",
    "\n",
    "# Padding\n",
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "x_data_pad = padding_(x_data, 500)\n",
    "lda_data_pad = padding_(lda_features, 5)  # Assuming LDA vectors have a length of 5\n",
    "\n",
    "# Combine the padded LDA data and tokenized text data\n",
    "combined_data = [list(lda_data_pad[i]) + list(x_data_pad[i]) for i in range(len(lda_data_pad))]\n",
    "\n",
    "# Create Tensor datasets\n",
    "y_data = y\n",
    "data = TensorDataset(torch.from_numpy(np.array(combined_data)), torch.from_numpy(y_data))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, input_dim, hidden_dim, embedding_dim):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.output_dim = 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = input_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim, num_layers=no_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Calculate the actual batch size\n",
    "        h0 = torch.zeros(self.no_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.no_layers, batch_size, self.hidden_dim).to(device)\n",
    "        hidden = (h0, c0)\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out\n",
    "\n",
    "model = SentimentRNN(no_layers, vocab_size, hidden_dim, embedding_dim)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Function to predict accuracy\n",
    "def acc(pred, label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "# Training and Evaluation\n",
    "clip = 5\n",
    "epochs = 6\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc, epoch_vl_acc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        accuracy = acc(output, labels)\n",
    "        train_acc += accuracy\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "            val_losses.append(val_loss.item())\n",
    "            accuracy = acc(output, labels)\n",
    "            val_acc += accuracy\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(test_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    print(f'train_loss: {epoch_train_loss} val_loss: {epoch_val_loss}')\n",
    "    print(f'train_accuracy: {epoch_train_acc * 100} val_accuracy: {epoch_val_acc * 100}')\n",
    "    print(25 * '==')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
