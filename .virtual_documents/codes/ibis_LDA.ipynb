import pandas as pd 
import numpy as np
import re

##LDA stuff
import gensim
from gensim.utils import simple_preprocess
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import gensim.corpora as corpora
import pyprojroot.here as here


df = pd.read_csv(here('data/cleaned/cleaned_ibis-sg-bencoolen.csv'))
df.head()


df_precovid = df[df['covid'] == 'PreCovid']
df_precovid.head()


# Initialize an empty list to store the analysis results
analysis = []

# Get the set of English stopwords using NLTK
stop_words = set(stopwords.words('english'))

# Loop through the rows of a DataFrame called 'df_precovid'
for i, r in df_precovid.iterrows():
    
    # Get the 'cleaned_review' column from the DataFrame and convert it to lowercase
    r_lower = r['cleaned_review'].lower()
    
    # Tokenize the lowercase text into words
    res = word_tokenize(r_lower)
    
    # Remove stopwords from the tokenized words and add the result to the 'analysis' list
    analysis.append([w for w in res if w not in stop_words])


# Create a Gensim dictionary from the 'analysis' list, which maps words to unique IDs
dictionary = corpora.Dictionary(analysis)

# Prepare the 'analysis' list for creating a bag-of-words corpus
words = analysis

# Create a bag-of-words representation of the 'words' using the dictionary
corpus = [dictionary.doc2bow(text) for text in words]

# Print the bag-of-words representation of the first document (corpus[0])
corpus[0]


lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=6)
print(lda_model.print_topics())


##visualisation
from matplotlib import pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.colors as mcolors

cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

cloud = WordCloud(background_color='white',
                  width=2500,
                  height=1800,
                  max_words=10,
                  colormap='tab10',
                 color_func=lambda *args, **kwargs: cols[i],
                  prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False)

fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)

##From https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/
for i, ax in enumerate(axes.flatten()):
    fig.add_subplot(ax)
    topic_words = dict(topics[i][1])
    cloud.generate_from_frequencies(topic_words, max_font_size=300)
    plt.gca().imshow(cloud)
    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))
    plt.gca().axis('off')
    
plt.subplots_adjust(wspace=0, hspace=0)
plt.axis('off')
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()



from collections import Counter
topics = lda_model.show_topics(formatted=False)
data_flat = [w for w_list in words for w in w_list]
counter = Counter(data_flat)

out = []
for i, topic in topics:
    for word, weight in topic:
        out.append([word, i , weight, counter[word]])

df_plot = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        

# Plot Word Count and Weights of Topic Keywords
fig, axes = plt.subplots(3, 2, figsize=(16,10), sharey=True, dpi=160)
cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]
for i, ax in enumerate(axes.flatten()):
    ax.bar(x='word', height="word_count", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')
    ax_twin = ax.twinx()
    ax_twin.bar(x='word', height="importance", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.2, label='Weights')
    ax.set_ylabel('Word Count', color=cols[i])
    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)
    ax.tick_params(axis='y', left=False)
    ax.set_xticklabels(df_plot.loc[df_plot.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')
    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')

fig.tight_layout(w_pad=2)    
fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    
plt.show()


# Initialize an empty list to store the analysis results
analysis = []

# Get the set of English stopwords using NLTK
stop_words = set(stopwords.words('english'))

# Loop through the rows of a DataFrame called 'df_precovid'
for i, r in df_precovid.iterrows():
    
    # Get the 'cleaned_review' column from the DataFrame and convert it to lowercase
    r_lower = r['cleaned_review'].lower()
    
    # Tokenize the lowercase text into words
    res = word_tokenize(r_lower)
    
    # Remove stopwords from the tokenized words and add the result to the 'analysis' list
    analysis.append([w for w in res if w not in stop_words])


# Create a Gensim dictionary from the 'analysis' list, which maps words to unique IDs
dictionary = corpora.Dictionary(analysis)

# Prepare the 'analysis' list for creating a bag-of-words corpus
words = analysis

# Create a bag-of-words representation of the 'words' using the dictionary
corpus = [dictionary.doc2bow(text) for text in words]

# Print the bag-of-words representation of the first document (corpus[0])
corpus[0]


lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=6)
print(lda_model.print_topics())


cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

cloud = WordCloud(background_color='white',
                  width=2500,
                  height=1800,
                  max_words=10,
                  colormap='tab10',
                 color_func=lambda *args, **kwargs: cols[i],
                  prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False)

fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)

##From https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/
for i, ax in enumerate(axes.flatten()):
    fig.add_subplot(ax)
    topic_words = dict(topics[i][1])
    cloud.generate_from_frequencies(topic_words, max_font_size=300)
    plt.gca().imshow(cloud)
    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))
    plt.gca().axis('off')
    
plt.subplots_adjust(wspace=0, hspace=0)
plt.axis('off')
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()



from collections import Counter
topics = lda_model.show_topics(formatted=False)
data_flat = [w for w_list in words for w in w_list]
counter = Counter(data_flat)

out = []
for i, topic in topics:
    for word, weight in topic:
        out.append([word, i , weight, counter[word]])

df_plot = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        

# Plot Word Count and Weights of Topic Keywords
fig, axes = plt.subplots(3, 2, figsize=(16,10), sharey=True, dpi=160)
cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]
for i, ax in enumerate(axes.flatten()):
    ax.bar(x='word', height="word_count", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')
    ax_twin = ax.twinx()
    ax_twin.bar(x='word', height="importance", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.2, label='Weights')
    ax.set_ylabel('Word Count', color=cols[i])
    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)
    ax.tick_params(axis='y', left=False)
    ax.set_xticklabels(df_plot.loc[df_plot.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')
    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')

fig.tight_layout(w_pad=2)    
fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    
plt.show()



