# src: https://www.kaggle.com/code/harshsingh2209/complete-guide-to-eda-on-text-data


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import string
import re
import nltk

from tqdm import trange
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer


import warnings
warnings.filterwarnings('ignore')
nltk.download('omw-1.4', quiet=True)
sns.set_style('darkgrid')
plt.rcParams['figure.figsize'] = (17,7)
plt.rcParams['font.size'] = 18


star3 = ['cleaned_ibis-sg-bencoolen.csv','cleaned_hotel-boss.csv','cleaned_hotel-G.csv',
           'cleaned_village-hotel-albert-court-by-far-east-hospitality.csv',
           'cleaned_holiday-inn-express-clarke-quay.csv']
star4 = ['cleaned_village-hotel-changi-by-far-east-hospitality.csv',
         'cleaned_park-regis.csv', 'cleaned_grand-mercure-sg-roxy.csv',
         'cleaned_paradox-sg-merchant-court.csv','cleaned_crowne-plaza.csv']
star5 = ['cleaned_fullerton.csv', 'cleaned_parkroyal-collection-marina-bay.csv', 'cleaned_pan-pacific.csv',
          'cleaned_mbs_total.csv', 'cleaned_swissotel-the-stamford.csv']


ROOT = r'C:\Nga\BT4222\sentiment_analysis_bt4222'
RAW_FOLDER = "data\\cleaned"


def combine_csv_to_dataframe(file_names):
    """
    Combine multiple CSV files into a single DataFrame.

    Parameters:
    file_names (list): List of CSV file names.

    Returns:
    pd.DataFrame: Combined DataFrame.
    """
    combined_df = pd.DataFrame()
    
    for file_name in file_names:
        file_name = ROOT + "\\"+RAW_FOLDER + "\\" + file_name
        try:
            df = pd.read_csv(file_name)
            combined_df = pd.concat([combined_df, df], ignore_index=True)
        except FileNotFoundError:
            print(f"File not found: {file_name}")
        except pd.errors.EmptyDataError:
            print(f"Empty or invalid CSV file: {file_name}")
    
    return combined_df


data = combine_csv_to_dataframe(star3)
data[['traveller_username','date','travel_type','traveller_total_contributions','traveller_total_helpful_contributions','review_title','review_text','rating']].head(5)


data.info()


data


#plt.pie(data['label'].value_counts(), labels=data['label'].unique().tolist(), autopct='%1.1f%%')
#plt.show()


# combine raw title and review
def combine(r):
  return str(r['review_title']) + " " + str(r['review_text'])


data["combined_review"] = data.apply(lambda row: combine(row), axis = 1)


data.head(3)


data['date'] = data['date_of_stay'].str.split(':').str[1]


timeline = data.groupby('date').count()['traveller_username']


timeline.index = pd.to_datetime(timeline.index)


timeline = timeline.sort_index()


timeline


plt.plot(timeline)
plt.title("Time against Number of Reviews")
plt.xlabel("Date")
plt.ylabel("Number of Reviews")


data


from datetime import date, timedelta, datetime


covid_start = datetime(2020, 1, 29, 0, 0)
covid_end = datetime(2022, 4, 1, 0, 0, 0)
def get_period(t):
    if pd.isnull(t):
        return None
    if t - covid_start < timedelta(0):
        return "PreCovid"
    elif t-covid_end >= timedelta(0):
        return "PostCovid"
    return "Covid"


data["date"] = pd.to_datetime(data["date"])


data["covid"] = data.apply(lambda row: get_period(row["date"]), axis = 1)


data


covid_data = data.dropna(axis=0, subset=['covid'])


covid_data['covid'].value_counts()


plt.pie(covid_data['covid'].value_counts(), labels=covid_data['covid'].value_counts().index.unique().tolist(), autopct='%1.1f%%')
plt.show()


length = len(data['cleaned_review'][0])
print(f'Length of a sample review: {length}')


data['Length'] = data['combined_review'].str.len()
data.head(10)


word_count = data['cleaned_review'][0].split()
print(f'Word count in a sample review: {len(word_count)}')


def word_count(review):
    review_list = review.split()
    return len(review_list)


data['Word_count'] = data['cleaned_review'].apply(word_count)
data.head(10)


data['mean_word_length'] = data['cleaned_review'].map(lambda rev: np.mean([len(word) for word in rev.split()]))
data.head(10)


nltk.download('punkt')


np.mean([len(sent) for sent in tokenize.sent_tokenize(data['combined_review'][0])])


data['mean_sent_length'] = data['cleaned_review'].map(lambda rev: np.mean([len(sent) for sent in tokenize.sent_tokenize(rev)]))
data.head(10)


def visualize(col):

    print()
    plt.subplot(1,2,1)
    sns.boxplot(y=data[col], hue=data['label'])
    plt.ylabel(col, labelpad=12.5)

    plt.subplot(1,2,2)
    sns.kdeplot(data = data, x = col, hue='label')
    plt.legend(data['label'].unique())
    plt.xlabel('')
    plt.ylabel('')

    plt.show()


data.columns


features = ['Length', 'Word_count', 'mean_word_length', 'mean_sent_length']
for feature in features:
    print(feature)
    visualize(feature)


df = data.drop(features, axis=1)
df.head()


df.info()


def clean(review):

    review = review.lower()
    review = re.sub('[^a-z A-Z 0-9-]+', '', review)
    review = " ".join([word for word in review.split() if word not in stopwords.words('english')])

    return review


nltk.download('stopwords')


#df['combined_review'] = df['combined_review'].apply(clean)
#df.head(10)


df['cleaned_review'][0]


def corpus(text):
    text_list = text.split()
    return text_list


df['Review_lists'] = df['cleaned_review'].apply(corpus)
df.head(10)


corpus = []
for i in trange(df.shape[0], ncols=150, nrows=10, colour='green', smoothing=0.8):
    corpus += df['Review_lists'][i]
len(corpus)


mostCommon = Counter(corpus).most_common(10)
mostCommon


words = []
freq = []
for word, count in mostCommon:
    words.append(word)
    freq.append(count)


sns.barplot(x=freq, y=words)
plt.title('Top 10 Most Frequently Occuring Words')
plt.show()


df


cv = CountVectorizer(ngram_range=(2,2))
bigrams = cv.fit_transform(df['cleaned_review'])


count_values = bigrams.toarray().sum(axis=0)
ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))
ngram_freq.columns = ["frequency", "ngram"]


sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])
plt.title('Top 10 Most Frequently Occuring Bigrams')
plt.show()


cv1 = CountVectorizer(ngram_range=(3,3))
trigrams = cv1.fit_transform(df['combined_review'])
count_values = trigrams.toarray().sum(axis=0)
ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv1.vocabulary_.items()], reverse = True))
ngram_freq.columns = ["frequency", "ngram"]


sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])
plt.title('Top 10 Most Frequently Occuring Trigrams')
plt.show()





good_reviews = df[df.label == "Positive"]
bad_reviews = df[df.label == "Negative"]


good_reviews_text = " ".join(good_reviews.combined_review.to_numpy().tolist())
bad_reviews_text = " ".join(bad_reviews.combined_review.to_numpy().tolist())


from wordcloud import WordCloud, STOPWORDS


stopwords = set(STOPWORDS)

# generate Word Cloud
def gen_wc(txt, cmap):
    w = ['hotel', 'room', 'rooms', 'location', 'staff', 'breakfast', 'bed', 'bathroom'] # selected words to be removed from wordcloud
    stopwords = set(STOPWORDS).union(w)

    # crisp wordcloud : https://stackoverflow.com/a/28795577/11105356
    wc = WordCloud(width=800, height=400,background_color="white", max_font_size=300, stopwords = stopwords, colormap = cmap).generate(txt)
    plt.figure(figsize=(14,10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis('off')
    plt.show()


gen_wc(good_reviews_text, "summer")


gen_wc(bad_reviews_text, "YlOrRd")



