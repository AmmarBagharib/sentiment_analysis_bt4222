import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic("matplotlib", " inline")
import seaborn as sns
import re
import nltk
import string

from sklearn.model_selection import train_test_split, KFold, cross_val_score

from sklearn.linear_model import LogisticRegression 
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, plot_confusion_matrix, balanced_accuracy_score

from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.base import BaseEstimator, TransformerMixin

from textblob import TextBlob
from wordcloud import WordCloud

from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.pipeline import Pipeline, make_pipeline

from sklearn_pandas import DataFrameMapper
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import Binarizer, FunctionTransformer 

#from pickle import dumps
from pickle import load

from dill import dumps as dp
from dill import loads as ld 
from cloudpickle import dump
import joblib

import warnings
warnings.filterwarnings('ignore')


#!pip install matplotlib==3.4


text_data = pd.read_excel('hotel_reviews.xlsx')
text_data.head(20)


# Checking the size of dataset
text_data.shape


#Summarizing the overall dataset
text_data.info()


#Checking the null values in dataset
text_data.isnull().sum()


#Checking the duplicate values
text_data.duplicated().sum()


#Statistical description of data (reviews)
text_data.describe(include='O')


#Statistical description of data (ratings)
text_data.describe()


#Unique rating values 
sorted(text_data['Rating'].unique())


#Count of Ratings
text_data['Rating'].value_counts()


#Converting the ratings column from float to int
text_data['Rating'] = text_data['Rating'].astype('int')


#Count of ratings after change in data-type
text_data['Rating'].value_counts()


#Visualizing the unique values of ratings
plt.figure(figsize=(8,5))

ax = sns.countplot(data = text_data , x = 'Rating', edgecolor='black', linewidth=1)

for i in ax.containers:
    ax.bar_label(i, label_type='center')
    
plt.title('Count of Ratings')
plt.show()


#Checking the total contribution of all ratings
plt.pie(text_data['Rating'].value_counts(), autopct='%0.2f', labels=['5', '4', '3', '2', '1'], explode=[0.03, 0.02, 0.0, 0.0, 0.0])
plt.title('Overall Ratings Distribution\n')
plt.show()


#Checking the most used words from reviews & plotting it using WordCloud
def wordCloud_generator(data, title=None):
    wordcloud = WordCloud(width = 1000, height = 750, max_words=300,
                          #background_color ='white',
                          min_font_size = 10
                         ).generate(" ".join(data.values))
                        
    plt.figure(figsize = (6, 5), facecolor = None) 
    plt.imshow(wordcloud, interpolation='bilinear') 
    plt.axis("off") 
    plt.tight_layout(pad = 0) 
    plt.title(title,fontsize=25)
    plt.show() 


#Plotting the wordcloud on original data for all ratings
wordCloud_generator(data=text_data['Review'], title="Most used words in reviews\n")


#Checking the most used words from reviews & plotting it using WordCloud
def wordCloud_generator(data, title=None):
    wordcloud = WordCloud(width = 1000, height = 750, max_words=300,
                          #background_color ='white',
                          min_font_size = 10
                         ).generate(" ".join(data.values))
                        
    plt.figure(figsize = (6, 5), facecolor = None) 
    plt.imshow(wordcloud, interpolation='bilinear') 
    plt.axis("off") 
    plt.tight_layout(pad = 0) 
    plt.title(title,fontsize=25)
    plt.show() 


#Plotting the wordcloud on original data for all highest rating (5)
wordCloud_generator(data=text_data[text_data['Rating']==5]['Review'], title="Most used words for highest rating\n")


#Checking the most used words from reviews & plotting it using WordCloud
def wordCloud_generator(data, title=None):
    wordcloud = WordCloud(width = 1000, height = 750, max_words=300,
                          #background_color ='white',
                          min_font_size = 10
                         ).generate(" ".join(data.values))
                        
    plt.figure(figsize = (6, 5), facecolor = None) 
    plt.imshow(wordcloud, interpolation='bilinear') 
    plt.axis("off") 
    plt.tight_layout(pad = 0) 
    plt.title(title,fontsize=25)
    plt.show() 


#Plotting the wordcloud on original data for all lowest rating (1)
wordCloud_generator(data=text_data[text_data['Rating']==1]['Review'],title="Most used words for lowest rating\n")


#Checking the length of words in a review for all the rating
plt.figure(figsize=(8,5))
sns.scatterplot(x=text_data['Review'].apply(len), y=text_data['Rating'], data=text_data, palette='crest', hue='Rating')
plt.title('Length of Review vs Rating')
plt.yticks([1,2,3,4,5])
plt.show()


#Making a copy of the original dataset
text_data1 = text_data.copy()


#Preview of dataset
text_data1


nltk.download('stopwords')


nltk.download('punkt')


nltk.download('wordnet')


nltk.download('omw-1.4')


#Object Instantiation
stop_words = nltk.corpus.stopwords.words('english')
stpw = ENGLISH_STOP_WORDS
wnl = WordNetLemmatizer()


# defined a function for cleaning all reviews in the data set

def clean_text(text):
    
    text = re.sub(r'\w*\d\w*', '', str(text)).strip() #Removing numeric attached with words and return only alpha 
    
    text = re.sub("[\d]+", "", str(text))  # Removing the strings which contains unnecessary digits in data
    
    text = re.sub(r"won\'t", "will not", str(text))      # Replace contraction words
    text = re.sub(r"can\'t", "can not", str(text))      # Replace contraction words
    text = re.sub(r"ca n\'t", "can not", str(text))      # Replace contraction words
    text = re.sub(r"wo n\'t", "will not", str(text))      # Replace contraction words
    text = re.sub(r"\'t've", " not have", str(text))     # Replace contraction words
    text = re.sub(r"\'d've", " would have", str(text))   # Replace contraction words
    text = re.sub(r"\'cause", " because", str(text))     # Replace contraction words
    
    text = re.sub(r"n\'t", " not", str(text))     # Replace contraction words
    text = re.sub(r"\'re", " are", str(text))     # Replace contraction words
    text = re.sub(r"\'s", " is", str(text))       # Replace contraction words
    text = re.sub(r"\'d", " would", str(text))    # Replace contraction words
    text = re.sub(r"\'ll", " will", str(text))    # Replace contraction words
    text = re.sub(r"\'t", " not", str(text))      # Replace contraction words
    text = re.sub(r"\'ve", " have", str(text))    # Replace contraction words
    text = re.sub(r"\'m", " am", str(text))       # Replace contraction words

    text = re.sub(r"n\'t", " not", str(text))     # Replace contraction words
    text = re.sub(r"\'re", " are", str(text))     # Replace contraction words
    text = re.sub(r"\'s", " is", str(text))       # Replace contraction words
    text = re.sub(r"\'d", " would", str(text))    # Replace contraction words
    text = re.sub(r"\'ll", " will", str(text))    # Replace contraction words
    text = re.sub(r"\'t", " not", str(text))      # Replace contraction words
    text = re.sub(r"\'ve", " have", str(text))    # Replace contraction words
    text = re.sub(r"\'m", " am", str(text))       # Replace contraction words
    
    text = re.sub(r'[^\w\s]', " ", str(text))    # Removing white spaces and returning only alpha characters
    
    text = text.translate(str.maketrans('','',string.punctuation)) # Remove Punctuations
    
    text = re.sub('[%s]' % re.escape(string.punctuation), '', str(text)) # Remove Punctuations
    
    text = ' '.join( [w for w in text.split() if len(w)>1] ) # Removing an unnecessary single character from sentences
    
    text = text.split() #Splitting each word
    
    text = " ".join([word for word in text if word.lower().strip() not in stop_words]) #Removing stopwords, white spaces, making words lower and joining into text
    
    text = text.split() # Splitting each word
    
    text = " ".join([word for word in text if word not in stpw]) # Removing extra stopwords if any
    
    text = nltk.word_tokenize(text)  # Tokenising each words 
       
    text = " ".join([wnl.lemmatize(w, 'v') for w in text])  # Applying Lemmatization

    text = ' '.join(dict.fromkeys(text.split())) #Remove duplicate words from a sentence
    
    return text


#Testing purpose - reviewing the original reviews before text preprocessing
text_data1['Review'][2]


#Testing purpose - Checking length of original data
len(text_data1['Review'][2])


#Testing purpose - reviewing the original reviews after text preprocessing
clean_text(text_data1['Review'][2])


#Testing purpose - Checking length of preprocessed data
len(clean_text(text_data1['Review'][2]))


#Applying the clean_text function on entire dataset
text_data1['Fully_Clean_Text'] = text_data1['Review'].apply(lambda x: clean_text(x))
text_data1


#Testing purpose - Verifying the sample review
len(text_data1['Fully_Clean_Text'][2])


text_data1['Length'] = text_data1['Review'].apply(len)
new_length = text_data1['Length'].sum()


text_data1['Lengths'] = text_data1['Fully_Clean_Text'].apply(len)
new_lengths = text_data1['Lengths'].sum()


#Visualizing the length of reviews
sns.barplot(data=text_data1, x='Rating', y='Length')
plt.title('Length of Reviews w.r.t Ratings\n')
plt.show()


#Comparing the length of reviews before & after text preprocessing
print('\033[1m'+ "Total Length of Review Comparison" + '\033[0m')
print('=======================================')
print("Before Text Preprocessing : {}".format(new_length))
print('---------------------------------------')
print("After Text Preprocessing  :{}".format(new_lengths))
print('=======================================')


##Object Instantiation
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()


#Defining a function for Sentiment Analysis
def fetch_sentiment_using_SIA(text):
    sia = SentimentIntensityAnalyzer()
    polarity_scores = sia.polarity_scores(text)
    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'


#Applying the VADER function to entire data
text_data1['SIA_Vader'] = text_data1.Fully_Clean_Text.apply(lambda x: fetch_sentiment_using_SIA(x))


#Reviewing the data
text_data1[['Fully_Clean_Text', 'Rating', 'SIA_Vader']]


#Drawn a positive sample for testing the sentiment
text_data1['Fully_Clean_Text'][20487]


#Testing the sample
sia.polarity_scores(text_data1['Fully_Clean_Text'][20487])


#Drawn a negative sample for testing the sentiment
text_data1['Fully_Clean_Text'][20489]


#Testing the sample
sia.polarity_scores(text_data1['Fully_Clean_Text'][20489])


#Visualizing the Vader Sentiment Analysis
plt.figure(figsize=(8,5))

plt.title("Visualizing the VADER Sentiment w.r.t Ratings")

ax = sns.countplot(data = text_data1 , x = 'Rating', hue = 'SIA_Vader')

for i in ax.containers:
    ax.bar_label(i,)
        
print('Total Count of Sentiments\n')

print(text_data1['SIA_Vader'].value_counts(['SIA_Vader'])*100, '\n')


#Applying the TextBlob to entire data
text_data1['TxtB_Polarity'] = text_data1['Fully_Clean_Text'].apply(lambda x: TextBlob(x).sentiment[0])


#Reviewing the data
text_data1[['Fully_Clean_Text', 'Rating', 'TxtB_Polarity']]


#Drawn a positive sample for testing the sentiment
txt = TextBlob(text_data1['Fully_Clean_Text'][20487])


#Testing the sample
txt.polarity


#Testing the sample
txt.sentiment_assessments


#Drawn a negative sample for testing the sentiment
txt2 = TextBlob(text_data1['Fully_Clean_Text'][20489])


#Testing the sample
txt2.polarity


#Testing the sample
txt2.sentiment_assessments


#Visualizing the TextBlob Sentiment Analysis
plt.figure(figsize=(8,5))

ax = sns.barplot(data= text_data1, x = 'Rating', y = 'TxtB_Polarity')

plt.title("Visualizing the TextBlob Sentiment w.r.t Ratings")

plt.show()


get_ipython().getoutput("pip install afinn")


# Emotion Lexicon - Affin
from afinn import Afinn


#Object Instantiation
afn = Afinn()


#Drawn a negative sample for testing the sentiment
nn = text_data1['Fully_Clean_Text'][20490]


#Testing the negative sample
afn.score(nn)


#Drawn a positive sample for testing the sentiment
pp = text_data1['Fully_Clean_Text'][20487]


#Testing the positive sample
afn.score(pp)


#Applying afinn to dataset
text_data1['Afinn_score'] = text_data1['Fully_Clean_Text'].apply(lambda x: afn.score(x))


#Reviewing the data
text_data1[['Fully_Clean_Text',  'Rating', 'Afinn_score']]


#Visualizing the Afinn Sentiment Analysis

plt.figure(figsize=(15,8))

ax = sns.violinplot(data= text_data1, x = 'Rating', y = 'Afinn_score')

plt.title("Visualizing the TextBlob Sentiment w.r.t Ratings")

plt.show()


#Defining a function for Sentiment Analysis
def condition(x):
    if x>=20:
        return "Pos"
    else:
        return 'Neg'


#Applying condition on Afinn score 
text_data1['Affin_Sent'] = text_data1['Afinn_score'].apply(condition)


#Reviewing the data
text_data1[['Fully_Clean_Text',  'Rating', 'Afinn_score', 'Affin_Sent']]


#Count of Afinn sentiment
text_data1['Affin_Sent'].value_counts()


#Analysing the data for rating = 1 with lowest score
text_data1[text_data1['Rating']==1].min()['Afinn_score']


#Analysing the data for rating = 1 with highest score
text_data1[text_data1['Rating']==1].max()['Afinn_score']


#Analysing the data for rating = 5 with lowest score
text_data1[text_data1['Rating']==5].min()['Afinn_score']


#Analysing the data for rating = 5 with lowest score
text_data1[text_data1['Rating']==5].max()['Afinn_score']


#Converting the ratings into two labels of Positive & Negative

text_data1['Target'] = text_data1['Rating'].apply(lambda x: 1 if x > 3 else 0)


#Reference for original data for encoded value
text_data1.head()


#Re-checking the dtype for all columns
text_data1.info()


#Checking the ratings
plt.figure(figsize=(12,6))

g = plt.pie(round(text_data1.Target.value_counts(normalize=True)*100,2),explode=(0.025,0.025), 
            labels=['Positive - 1', 'Negative - 0'], colors=["c","m"],
            autopct="%1.1f%%", startangle=180)

plt.title("% Distributions by Review Type")

plt.show()


#TF-IDF features
vectorizer = TfidfVectorizer()


#Splitting the variables into features & target
X = text_data1['Fully_Clean_Text']
y = text_data1['Target']


#Checking the count of target variable
text_data1['Target'].value_counts()


#Segregating data into train & test
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3, shuffle=True, stratify=y)


#Applying the TF-IDF vectorizer to train data
X_train = vectorizer.fit_transform(X_train)


#Applying the TF-IDF vectorizer to test data
X_test = vectorizer.transform(X_test)


#Building the different classification models with default parameters
lr_mod  = LogisticRegression(random_state=0, max_iter=1000, class_weight='balanced')
nb_mod  = MultinomialNB()


get_ipython().run_cell_magic("time", "", """lr_mod.fit(X_train, y_train)
nb_mod.fit(X_train, y_train)""")


get_ipython().run_cell_magic("time", "", """y_pred_lr  = lr_mod.predict(X_test)
y_pred_nb = nb_mod.predict(X_test)""")


#Checking the count of test variable - target
y_test.value_counts()


#Checking the accuracy for all models
cc  = [y_pred_lr, y_pred_nb]
mod = ['Logistic Regression', 'Naive Bayes']

print('Accuracy Scores for all models','\n')

for i,j in zip(cc, mod):
    print('===========================')
    print(j, ':', balanced_accuracy_score(y_test, i).round(2))
    print('===========================', '\n')


#Plotting Confusion Matrix for all models
dd  = [lr_mod, nb_mod]
mod = ['Logistic Regression', 'Naive Bayes']

print('Confusion Matrix for all models')

for i,j in zip(dd, mod):
    print('\n','========================================', '\n')
    print(j)
    plot_confusion_matrix(i, X_test, y_test, display_labels= ['Neg', 'Pos'])   
    plt.show()


#Classification Report for all models
cc  = [y_pred_lr, y_pred_nb]
mod = ['Logistic Regression', 'Naive Bayes']

print('Classification Report for all models','\n')

for i,j in zip(cc, mod):
    print('========================================================')
    print(j,'\n\n' , classification_report(y_test, i))
    print('========================================================', '\n')


#Transforming the data to X & y
X = vectorizer.fit_transform(X)
y = y


#Object Instantiation
sm = SMOTE(random_state=0)


#Resampling the dataset with smote
X_res, y_res = sm.fit_resample(X, y)


#Rechecking the count of target variable after smote
print('X_res after resampling shape : ' , X_res.shape)
print('y_res after resampling shape : ' , y_res.shape, '\n')
print(y_res.value_counts())


#Splitting data into training and testing dataset with 80:20 ratio
X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res, random_state=0, test_size=0.35, stratify=y_res)


#Checking the size of features
X_train_res.shape, X_test_res.shape


lr_mod2  =LogisticRegression(random_state=0, max_iter=1000)
nb_mod2  = MultinomialNB()


get_ipython().run_cell_magic("time", "", """lr_mod2.fit(X_train_res, y_train_res)
nb_mod2.fit(X_train_res, y_train_res)""")


get_ipython().run_cell_magic("time", "", """y_pred_lr2  = lr_mod2.predict(X_test_res)
y_pred_nb2  = nb_mod2.predict(X_test_res)""")


#Checking the accuracy for all models
cc  = [y_pred_lr2, y_pred_nb2]
mod = ['Logistic Regression', 'Naive Bayes']

print('Accuracy Scores for all models','\n')

for i,j in zip(cc, mod):
    print('===========================')
    print(j, ':', balanced_accuracy_score(y_test_res, i).round(2))
    print('===========================', '\n')


#Plotting Confusion Matrix for all models
mm = ['Logistic Regression', 'Naive Bayes']
dd = [lr_mod2 ,nb_mod2]

print('Confusion Matrix for all models')

for i,j in zip(dd, mm):
    print('\n','========================================', '\n')
    print(j)
    plot_confusion_matrix(i, X_test_res, y_test_res, display_labels= ['Neg', 'Pos'])   
    plt.show()


#Classification Report for all models
cc = [y_pred_lr2, y_pred_nb2]
mm = ['Logistic Regression', 'Naive Bayes']

for i,j in zip(cc, mm):
    print('======================================================')
    print(j, '\n', classification_report(y_test_res, i))
    print('======================================================', '\n')


#Comparing the models imbalance & model smote_balance data with balanced accuracy score
mod_comp = pd.DataFrame(index=mm, columns=['Model1', 'Model2'], data={'Model1': [balanced_accuracy_score(y_test ,y_pred_lr).round(2), balanced_accuracy_score(y_test ,y_pred_nb).round(2)], 
                                                                      'Model2': [balanced_accuracy_score(y_test_res ,y_pred_lr2).round(2), balanced_accuracy_score(y_test_res ,y_pred_nb2).round(2)]
                                                                       })
mod_comp['%Change in Accuracy'] = (mod_comp['Model2'] - mod_comp['Model1'])*100
mod_comp.sort_values(by='Model2', ascending=False)


#Object Instantiation
ad = ADASYN(random_state=0)


#Resampling the dataset with adasyn
X_ad, y_ad = ad.fit_resample(X, y)


#Rechecking the count of target variable after adasyn
print('X_res after resampling shape : ' , X_ad.shape)
print('y_res after resampling shape : ' , y_ad.shape, '\n')
print(y_ad.value_counts())


#Splitting data into training and testing dataset with 80:20 ratio
X_train_ad, X_test_ad, y_train_ad, y_test_ad = train_test_split(X_ad, y_ad, random_state=0, test_size=0.35, stratify=y_ad)


#Checking the size of features
X_train_ad.shape, X_test_ad.shape


lr_mod3  = LogisticRegression(random_state=0, max_iter=1000)
nb_mod3  = MultinomialNB()


get_ipython().run_cell_magic("time", "", """lr_mod3.fit(X_train_ad, y_train_ad)
nb_mod3.fit(X_train_ad, y_train_ad)""")


get_ipython().run_cell_magic("time", "", """y_pred_lr3  = lr_mod3.predict(X_test_ad)
y_pred_nb3  = nb_mod3.predict(X_test_ad)""")


#Testing the roc-auc score for all models
cc = [y_pred_lr3, y_pred_nb3]
mm = ['Logistic Regression', 'Naive Bayes']

print('ROC-AUC score for all models')

for i,j in zip(cc, mm):
    print('\n==============================')
    print(j, ':', balanced_accuracy_score(y_test_ad, i).round(2))
    print('==============================', '\n')


#Plotting Confusion Matrix for all models
mm = ['Logistic Regression', 'Naive Bayes']
dd = [lr_mod3, nb_mod3]

print('Confusion Matrix for all models')

for i,j in zip(dd, mm):
    print('\n','========================================', '\n')
    print(j)
    plot_confusion_matrix(i, X_test_ad, y_test_ad, display_labels= ['Negative', 'Positive'])   
    plt.show()


#Classification Report for all models
cc = [y_pred_lr3, y_pred_nb3]
mm = ['Logistic Regression', 'Naive Bayes']

for i,j in zip(cc, mm):
    print('======================================================')
    print(j, '\n', classification_report(y_test_ad, i))
    print('======================================================', '\n')


#Comparing the models imbalance & model adasyn_balance data with balanced accuracy score
mod_comp = pd.DataFrame(index=mm, columns=['Model1', 'Model2'], data={'Model1': [balanced_accuracy_score(y_test ,y_pred_lr).round(2), balanced_accuracy_score(y_test ,y_pred_nb).round(2)], 
                                                                      'Model2': [balanced_accuracy_score(y_test_ad ,y_pred_lr3).round(2), balanced_accuracy_score(y_test_ad ,y_pred_nb3).round(2)]
                                                                       })
mod_comp['%Change in Accuracy'] = mod_comp['Model2'] - mod_comp['Model1']
mod_comp.sort_values(by='Model2', ascending=False)


#Object Instantiation
rs = RandomUnderSampler(random_state=0)


#Resampling the dataset with random-under-sampler
X_rs, y_rs = rs.fit_resample(X, y)


#Rechecking the count of target variable after random-under-sampler
print('X_res after resampling shape : ' , X_rs.shape)
print('y_res after resampling shape : ' , y_rs.shape, '\n')
print(y_rs.value_counts())


#Splitting data into training and testing dataset with 80:20 ratio
X_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(X_rs, y_rs, random_state=0, test_size=0.35, stratify=y_rs)


#Checking the size of features
X_train_rs.shape, X_test_rs.shape


lr_mod4  = LogisticRegression(random_state=0, max_iter=1000)
nb_mod4  = MultinomialNB()


get_ipython().run_cell_magic("time", "", """lr_mod4.fit(X_train_rs, y_train_rs)
nb_mod4.fit(X_train_rs, y_train_rs)""")


get_ipython().run_cell_magic("time", "", """y_pred_lr4  = lr_mod4.predict(X_test_rs)
y_pred_nb4  = nb_mod4.predict(X_test_rs)""")


#Testing the balanced accuracy for all models
cc = [y_pred_lr4, y_pred_nb4]
mm = ['Logistic Regression', 'Naive Bayes']

print('Balanced Accuracy score for all models')

for i,j in zip(cc, mm):
    print('\n==============================')
    print(j, ':', balanced_accuracy_score(y_test_rs, i).round(2))
    print('==============================', '\n')


#Plotting Confusion Matrix for all models
mm = ['Logistic Regression', 'Naive Bayes']
dd = [lr_mod4, nb_mod4]

print('Confusion Matrix for all models')

for i,j in zip(dd, mm):
    print('\n','========================================', '\n')
    print(j)
    plot_confusion_matrix(i, X_test_rs, y_test_rs, display_labels= ['Negative', 'Positive'])   
    plt.show()


#Classification Report for all models
cc = [y_pred_lr4, y_pred_nb4]
mm = ['Logistic Regression', 'Naive Bayes']

for i,j in zip(cc, mm):
    print('======================================================')
    print(j, '\n', classification_report(y_test_rs, i))
    print('======================================================', '\n')


#Comparing the models imbalance & model random-under-sampler_balance data with balanced accuracy score
mod_comp = pd.DataFrame(index=mm, columns=['Model1', 'Model2'], data={'Model1': [balanced_accuracy_score(y_test ,y_pred_lr).round(2), balanced_accuracy_score(y_test ,y_pred_nb).round(2)], 
                                                                      'Model2': [balanced_accuracy_score(y_test_rs ,y_pred_lr4).round(2), balanced_accuracy_score(y_test_rs ,y_pred_nb4).round(2)]
                                                                       })
mod_comp['%Change in Accuracy'] = (mod_comp['Model2'] - mod_comp['Model1'])*100
mod_comp.sort_values(by='Model2', ascending=False)


#Object Instantiation
tl = TomekLinks()


#Resampling the dataset with smote-tomek
X_tl, y_tl = tl.fit_resample(X, y)


##Rechecking the count of target variable after tomek
print('X_res after resampling shape : ' , X_tl.shape)
print('y_res after resampling shape : ' , y_tl.shape, '\n')
print(y_tl.value_counts())


#Splitting data into training and testing dataset with 80:20 ratio
X_train_tl, X_test_tl, y_train_tl, y_test_tl = train_test_split(X_tl, y_tl, random_state=0, test_size=0.35, stratify=y_tl)


#Checking the size of features
X_train_tl.shape, X_test_tl.shape


lr_mod5  = LogisticRegression(random_state=0, max_iter=1000)
nb_mod5  = MultinomialNB()


get_ipython().run_cell_magic("time", "", """lr_mod5.fit(X_train_tl, y_train_tl)
nb_mod5.fit(X_train_tl, y_train_tl)""")


get_ipython().run_cell_magic("time", "", """y_pred_lr5  = lr_mod5.predict(X_test_tl)
y_pred_nb5  = nb_mod5.predict(X_test_tl)""")


#Testing the accuracy score for all models
cc = [y_pred_lr5, y_pred_nb5]
mm = ['Logistic Regression', 'Naive Bayes']

print('Balanced Accuracy score for all models')

for i,j in zip(cc, mm):
    print('\n==============================')
    print(j, ':', balanced_accuracy_score(y_test_tl, i).round(2))
    print('==============================', '\n')


#Plotting Confusion Matrix for all models
mm = ['Logistic Regression', 'Naive Bayes']
dd = [lr_mod5, nb_mod5]

print('Confusion Matrix for all models')

for i,j in zip(dd, mm):
    print('\n','========================================', '\n')
    print(j)
    plot_confusion_matrix(i, X_test_tl, y_test_tl, display_labels= ['Negative', 'Positive'])   
    plt.show()


#Classification Report for all models
cc = [y_pred_lr5, y_pred_nb5]
mm = ['Logistic Regression', 'Naive Bayes']

for i,j in zip(cc, mm):
    print('======================================================')
    print(j, '\n', classification_report(y_test_tl, i))
    print('======================================================', '\n')


#Comparing the models imbalance & model smote_balance data with balanced accuracy score
mod_comp = pd.DataFrame(index=mm, columns=['Model1', 'Model2'], data={'Model1': [balanced_accuracy_score(y_test ,y_pred_lr).round(2), balanced_accuracy_score(y_test ,y_pred_nb).round(2)], 
                                                                      'Model2': [balanced_accuracy_score(y_test_tl ,y_pred_lr5).round(2), balanced_accuracy_score(y_test_tl ,y_pred_nb5).round(2)]
                                                                       })
mod_comp['%Change in Accuracy'] = (mod_comp['Model2'] - mod_comp['Model1'])*100
mod_comp.sort_values(by='Model2', ascending=False)


kf = KFold(n_splits=15, shuffle=True, random_state=0)


cross_val_score(lr_mod, X, y, cv=kf, scoring='accuracy').mean().round(3)*100


cross_val_score(lr_mod, X, y, cv=kf, scoring='accuracy').std().round(3)*100


cross_val_score(nb_mod, X, y, cv=kf, scoring='accuracy').mean().round(3)*100


cross_val_score(nb_mod, X, y, cv=kf, scoring='accuracy').std().round(3)*100


cross_val_score(lr_mod2, X_res, y_res, cv=kf, scoring='accuracy').mean().round(3)*100


cross_val_score(lr_mod2, X_res, y_res, cv=kf, scoring='accuracy').std().round(3)*100


cross_val_score(nb_mod2, X_res, y_res, cv=kf, scoring='accuracy').mean().round(3)*100


cross_val_score(nb_mod2, X_res, y_res, cv=kf, scoring='accuracy').std().round(3)*100


#!pip install transformers


#distilbert-base-uncased-finetuned-sst-2-english


#Import required libraries
from transformers import AutoTokenizer, pipeline, TFDistilBertForSequenceClassification,
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification 
from transformers import DistilBertModel, DistilBertConfig, TFTrainer
from transformers import DistilBertTokenizer, TFPreTrainedModel
from scipy.special import softmax
import torch


#Model Building 
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
#tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)
classifier1 = pipeline('sentiment-analysis', model=tf_model, tokenizer=tokenizer)


#Model Evaluation
classifier1


#Testing the model - sarcastic sentence
classifier1('The bed was in such a good condition, I slept on the floor.')


#Testing the model - sarcastic sentence
classifier1('the food will bring nausea and motion sickness; it was that deliciously sick.')


#Testing the model - sarcastic sentence
classifier1('It was a fucking amazing experience!! Will definitely visit this hotel again.')


#Testing the model - negative sentence
classifier1('I am not feeling well.')


#Saving the model
classifier1.save_pretrained("mod_tf")


def clean_text_frame(X):
    b = clean_text
    return X.applymap(b)  # the function "clean_text" currently in your class.


#Using the mapper to perform specific transformer to the features
m = DataFrameMapper([(['Review'], FunctionTransformer(func=clean_text_frame)),
                      
                    ], df_out=True, input_df=True)


#Construct the column transfomer
vectorizer1 = TfidfVectorizer()
column_transformer = ColumnTransformer(
    [('Tfidf1', vectorizer1, 'Review')],
    remainder='passthrough')


#Object Instantiation
mod = LogisticRegression(random_state=0, max_iter=1300)

mod1 = MultinomialNB()

sampling  = SMOTE(random_state=0)


#Making Pipeline for Logistic Regression
model = Pipeline([ ('Cleaning', m),
                  ('Tfidf', column_transformer), ('SMOTE', sampling), ('MODEL', mod)
                ])


#Reviewing the pipeline
model


#Making Pipeline for Naive Bayes
model1 = Pipeline([ ('Cleaning', m),
                  ('Tfidf', column_transformer), ('SMOTE', sampling), ('MODEL', mod1)
                ])


#Reviewing the pipeline
model1


#Making another copy of original data 
text_data2 = text_data.copy()


#Label encoding the target variable
text_data2['Rating'] = text_data2['Rating'].apply(lambda x: 1 if x > 3 else 0)


#Reviewing the data after label encoding
text_data2.head()


#Picking up 20 random samples from the original data for testing
usx = text_data2.sample(n=20, random_state=12)
usx['Rating'].value_counts()


#Index values of samples
usix = usx['Rating'].index


#Splititing the collected samples for testing
X_t   = usx[['Review']]
y_t   = usx[['Rating']].values


X_t.shape, y_t.shape


#Dropping the collected samples from original dataset so that model is not trained on it
text_data2 = text_data2.drop(index=usix).reset_index()
text_data2 = text_data2.drop('index', axis=1)


#Reviewing the data after dropping the test samples
text_data2


#Splitting the original data for training the model
X_tr = text_data2[['Review']]
y_tr = text_data2[['Rating']].values


X_tr.head()


y_tr


#Training the model using Logistic Regression
model.fit(X_tr, y_tr)


#Predicting the test sample
model.predict(X_t)


#Model performance score
model.score(X_t, y_t)


#Model Testing & Evaluation - Logistic Regression

print('Actual data:', '   ',y_t.flatten(), '\n')

print('Predicted data: ', model.predict(X_t))


#model1 = model.fit_resample(X, y)


#Training the model using Naive Bayes
model1.fit(X_tr, y_tr)


#Predicting the test sample
model1.predict(X_t)


#Model performance score
model1.score(X_t, y_t)


#Model Testing & Evaluation - Naive Bayes

print('Actual data:', '   ',y_t.flatten(), '\n')

print('Predicted data: ', model1.predict(X_t))


#Custom reviews for testing after saving model
tt = {'Review': ['The bed was in such a good condition, I slept on the floor.', 'the food will bring nausea and motion sickness; it was that delicious sick.', 'It was a fucking amazing experience!! Will definitely visit this hotel again.']}
t1 = pd.DataFrame(tt, index=[0,1,2])
t1


#Saving a joblib model
joblib.dump(model1, 'joblib_model1')


#Loading a joblib model
mj = joblib.load('joblib_model1')


#Predicting the model based on custom reviews
mj.predict(t1)


#Saving the model to file
dump(model1, open('hrsa_intelligence.joblib', 'wb'))


#Loading the file
ml = load(open('hrsa_intelligence.joblib', 'rb'))


#Running the file for testing & evaluating custom reviews
print('Predicted Values:', ml.predict(t1))
